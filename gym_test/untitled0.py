# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wsHpPBYe5mcoCvpcS-Tkuz1RFitcNoOZ
"""

# Commented out IPython magic to ensure Python compatibility.
# Stable Baselines only supports tensorflow 1.x for now
# %tensorflow_version 1.x


import stable_baselines
stable_baselines.__version__

from gridworldRNN import *
from gridworld2 import *
from random import random
from gym import Env
import gym
from gym import spaces

from stable_baselines import PPO2
# check my gym enviroment
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.policies import MlpLstmPolicy

from stable_baselines.common.vec_env import SubprocVecEnv
from stable_baselines.common import set_global_seeds, make_vec_env
from stable_baselines.common.evaluation import evaluate_policy
from stable_baselines.common.policies import *

from stable_baselines.common.env_checker import check_env

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
# sns test
sns.set_style("darkgrid")

from stable_baselines import PPO2
# check my gym enviroment
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.policies import MlpLstmPolicy

from stable_baselines.common.vec_env import SubprocVecEnv
from stable_baselines.common import set_global_seeds, make_vec_env
from stable_baselines.common.evaluation import evaluate_policy
from stable_baselines.common.policies import *

from stable_baselines.common.env_checker import check_env

import matplotlib.pyplot as plt
import seaborn as sns
# sns test
sns.set_style("darkgrid")

def set_enviroment_rnn(n_width, n_height):
    

    env = GridWorldEnvRnn(n_width=n_width, n_height=n_height, u_size=60, default_type=0,
                             max_episode_steps=60, default_reward=-1)
   
 
    env.start= (2,2)
    env.end= (5,5)
  
    env.refresh_setting()
    check_env(env)
    return env

"""## calculate the standard deviation through running parallel"""

# add evaluation module
def evaluate(model, num_episodes=100):
    """
    Evaluate a RL agent
    :param model: (BaseRLModel object) the RL Agent
    :param num_episodes: (int) number of episodes to evaluate it
    :return: (float) Mean reward for the last num_episodes
    """
    # This function will only work for a single Environment
    env = model.get_env()
    all_episode_rewards = []
    for i in range(num_episodes):
        episode_rewards = []
        done = False
        obs = env.reset()
        while not done:
            # _states are only useful when using LSTM policies
            action, _states = model.predict(obs)
            #print("without noise:", action)
            # here, action, rewards and dones are arrays
            obs, reward, done, info = env.step(action)
            episode_rewards.append(reward)

        all_episode_rewards.append(sum(episode_rewards))

    mean_episode_reward = np.mean(all_episode_rewards)
    #print("Mean reward:", mean_episode_reward, "Num episodes:", num_episodes)

    return mean_episode_reward

def reward_plot(reward):
  x=np.linspace(0, 1000*(len(reward)),len(reward),endpoint=True)
  plt.plot(x,reward,label='model1')

def reward_plot_two(reward1,reward2):
    x=np.linspace(0, 1000*(len(reward1)),len(reward1),endpoint=True)
    plt.plot(x,reward1,label='model1')
    plt.plot(x,reward2)

def reward_plot_all(rewards):
  n = len(rewards)
  x=np.linspace(0, 1000*(len(rewards[0])-1),len(rewards[0])-1,endpoint=True)
  for _ in range(n):
    plt.plot(x, rewards[_][1:])

def train_agent(model):
  start_time = time.time()
  reward=[]
  for _ in range(10):
    model.learn(total_timesteps=1000)
    x = evaluate(model,num_episodes=100)
    reward.append(x)
  end_time = time.time()
  process_time =  end_time - start_time
  print("the process time is:", process_time)
  return reward,process_time

def train_plot(model,num):

  rewards = []
  ts = 0
  for _ in range(num):
    r,t = train_agent(model)
    rewards+=r
    ts+=t
    print(rewards[-1])
  
   
  return rewards

"""## Use the Muti-enviroments to calculate the standard deviation

"""

def make_env(n_width, n_height, rank,flag=False, seed=0):
    """
    Utility function for multiprocessed env.

    :param num_env: (int) the number of environments you wish to have in subprocesses
    :param seed: (int) the inital seed for RNG
    :param rank: (int) index of the subprocess
    """
    def _init():
        env = set_enviroment_rnn(n_width, n_height,flag)
        env.seed(seed + rank)
        return env
    set_global_seeds(seed)
    return _init

def condition_terminate(num_done, done):
  """
  it decides if we should end one episode
  the condition to terminate: all the enviroments have reached the destination
  """
  # num_done = np.array([False for i in range(4)])

  if done.any():
    item = np.where(done == True)
    # print("done:",done)
    # print("ietm:",item)
    #print(item[0])
    num_done[item[0]]=True
  # print("num_done",num_done)
  if num_done.all():
    flag = False
  else:
    flag = True
  
  return flag

def helper_reward(rewards,Done):
  """
  recalculate the reward for each enviroment
  the shape of rewards and Done should be same
  """
  
  assert rewards.shape==Done.shape,"the shape of rewards and Done is different"
  n,m = rewards.shape # m is the num of the parallel enviroments, n is the largest rewards(-1) in the parallel enviroments
  for i in range(m):
    if Done[:,i].any():
      itemindex = np.argwhere(Done[:,i]==True)[0][0]
      Done[:itemindex,i]= True
      if itemindex < n: Done[itemindex+1:,i]=False

  rewards= np.multiply(rewards,Done)
  sum_reward = np.sum(rewards,axis=0)
  mean_reward = np.mean(sum_reward)

  return mean_reward,sum_reward

def evaluate_multi(model, num_episodes=20):
    """
    Evaluate a RL agent
    :param model: (BaseRLModel object) the RL Agent
    :param num_episodes: (int) number of episodes to evaluate it
    :return: (float) Mean reward for the last num_episodes
    """
   
    env = model.get_env()
    num_envs = env.num_envs
    all_episode_rewards = []
    episode_rewards_env=[]

    for i in range(num_episodes):
      num_done = np.array([False for i in range(num_envs)]) # count if all the enviroment have reached the destination
      episode_rewards = []
      Done = []
      obs = env.reset()
      steps=0
      flag=True
      while flag:
          # _states are only useful when using LSTM policies
          action, _states = model.predict(obs)
          #print("without noise:", action)
          # here, action, rewards and dones are arrays
          obs, reward, done, info = env.step(action)
          episode_rewards.append(reward)
          Done.append(done)
          steps+=1
          flag = condition_terminate(num_done,done)

      episode_rewards = np.array(episode_rewards).reshape(-1,num_envs)
      Done = np.array(Done).reshape(-1,num_envs)
      mean_episode_rewards,sum_episode_rewards = helper_reward(rewards=episode_rewards, Done=Done)

      all_episode_rewards.append(mean_episode_rewards)
      episode_rewards_env.append(sum_episode_rewards)
      #print("the steps for one round:", steps)

    mean_episode_reward = np.mean(all_episode_rewards)
    #print("Mean reward:", mean_episode_reward, "Num episodes:", num_episodes)
    print(episode_rewards_env)

    return mean_episode_reward

# generate model with parameters

def generate_model(n_lstm=256,lstm_layers=12,vf=[128,64], pi=[128,64],
                  ent_coef=0.01, learning_rate=2.5e-4, vf_coef=0.5,cliprange=0.2,cliprange_vf=None):
  
  env_10_pomdp = set_enviroment_rnn(n_width=10, n_height=10)

  class CustomLSTMPolicy(LstmPolicy):
      """
  for the default LstmPolicy :
    n_lstm:(int) the number of LSTM cells (default = 256) the number of hidden neurons
    layers:([int]) The size of the Neural network before the LSTM layer  (if None, default to [64, 64])
    net_arch:(list) Specification of the actor-critic policy network architecture. Notation similar to the
        format described in mlp_extractor but with additional support for a 'lstm' entry in the shared network part.
  """
      def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, layers=None, n_lstm=n_lstm, reuse=False, **_kwargs):
        super().__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, layers, reuse,
                         net_arch=[lstm_layers, 'lstm', dict(vf=vf, pi=pi)],
                         layer_norm=True, feature_extraction="mlp", **_kwargs)


  
  model = PPO2(CustomLSTMPolicy,env_10_pomdp,gamma=0.99, n_steps=128, ent_coef=ent_coef, learning_rate=learning_rate, vf_coef=vf_coef,
                 max_grad_norm=0.5, lam=0.95, nminibatches=1, noptepochs=4, cliprange=cliprange, cliprange_vf=cliprange_vf,
                 verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None,
                 full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None )
  
  return model

# model 1:
def generate_train(num=4):
  rewards=[]
  plot_x=[]
  for i in range(num):
    model1 = generate_model(n_lstm=300,lstm_layers=14,vf=[300,64], pi=[300,64],
                  ent_coef=0.01, learning_rate=1.5e-4, vf_coef=0.5,cliprange=0.2,cliprange_vf=None)

    reward1 = train_plot(model1,num=10)
    x=[i*1000 for i in range(len(reward1))]
    rewards.append(reward1)
    plot_x.append(x)
  
  d={'x':plot_x, 'y':rewards}
  data = pd.DataFrame(d)

  return rewards, data

#rewards, data = generate_train(num=4)

#sns.lineplot(data=data, x="x", y="y",ci='sd')

# model1 = generate_model(n_lstm=300,lstm_layers=14,vf=[300,64], pi=[300,64],
#                   ent_coef=0.01, learning_rate=1.5e-4, vf_coef=0.5,cliprange=0.2,cliprange_vf=None)
#
# reward1 = train_plot(model1,num=10)